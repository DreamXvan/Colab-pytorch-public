{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFNQecSXJXZAjkU74KPVJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DreamXvan/Colab-pytorch-public/blob/main/Just_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JGCbeoCByUwQ"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jrQ9bYwyl8L",
        "outputId": "2339bd89-52f5-4215-9d21-c10a5d79cdf4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aciUaEyyuIl",
        "outputId": "35e22c8e-c02e-4c65-c595-16100d4b1f1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2S_p31zzdrt",
        "outputId": "5d63641b-36d6-4c55-ab31-31f403872f82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 16 14:16:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--device\", type=str, default=\"cuda:3\")\n",
        "parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
        "parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
        "parser.add_argument('--epochs', type=int, default=20, help='number of epochs')\n",
        "args = parser.parse_args()\n",
        "\n",
        "data_path = 'data'\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes, image_size, channel):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(channel, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.GroupNorm(128, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.GroupNorm(128, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.GroupNorm(128, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # 计算池化后的特征图大小\n",
        "        reduced_size = image_size // (2 ** 3)  # 三次 avgpool each downsample by 2\n",
        "        flattened_dim = 128 * reduced_size * reduced_size  # 特征图展开成一维\n",
        "\n",
        "        self.classifier = nn.Linear(flattened_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def epoch(dataloader, net, optimizer, criterion, args):\n",
        "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
        "    net = net.to(args.device)\n",
        "    criterion = criterion.to(args.device)\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for i_batch, datum in enumerate(dataloader):\n",
        "        img = datum[0].float().to(args.device)\n",
        "        lab = datum[1].long().to(args.device)\n",
        "        n_b = lab.shape[0]\n",
        "\n",
        "        output = net(img)\n",
        "        loss = criterion(output, lab)\n",
        "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
        "\n",
        "        loss_avg += loss.item() * n_b\n",
        "        acc_avg += acc\n",
        "        num_exp += n_b\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_avg /= num_exp\n",
        "    acc_avg /= num_exp\n",
        "\n",
        "    return loss_avg, acc_avg\n",
        "\n",
        "\n",
        "def test_acc(net, test_loader, args):\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(args.device), labels.to(args.device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train(train_loader, test_loader, args):\n",
        "    net = ConvNet(10, 28, 1).to(args.device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr_net, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    for e in range(0, args.epochs + 1):\n",
        "        if e % 10 == 0 or e == args.epochs:\n",
        "            acc = test_acc(net, test_loader, args)\n",
        "            print(f'{e} / {args.epochs}, test acc: {acc:.3f}')\n",
        "\n",
        "        train_loss, train_acc = epoch(train_loader, net, optimizer, criterion, args)\n",
        "        print(f'{e} / {args.epochs}, train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}')\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def hold_out(ratio, args):\n",
        "    mean = [0.1307]\n",
        "    std = [0.3081]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "    dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "    dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "    full_dataset = torch.utils.data.ConcatDataset([dst_train, dst_test])\n",
        "\n",
        "    targets = []\n",
        "    for d in full_dataset.datasets:\n",
        "        targets.extend(d.targets.numpy() if hasattr(d.targets, 'numpy') else d.targets)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=ratio, random_state=42)\n",
        "    indices = np.arange(len(full_dataset))\n",
        "    train_idx, test_idx = next(splitter.split(indices, targets))\n",
        "\n",
        "    train_subset = Subset(full_dataset, train_idx)\n",
        "    test_subset = Subset(full_dataset, test_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=args.batch_train, shuffle=True)\n",
        "    test_loader = DataLoader(test_subset, batch_size=args.batch_train, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def k_fold(k, args):\n",
        "    mean = [0.1307]\n",
        "    std = [0.3081]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "    dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "    dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "    full_dataset = torch.utils.data.ConcatDataset([dst_train, dst_test])\n",
        "\n",
        "    targets = []\n",
        "    for d in full_dataset.datasets:\n",
        "        targets.extend(d.targets.numpy() if hasattr(d.targets, 'numpy') else d.targets)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    indices = np.arange(len(full_dataset))\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_idx, test_idx in skf.split(indices, targets):\n",
        "        train_subset = Subset(full_dataset, train_idx)\n",
        "        test_subset = Subset(full_dataset, test_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size=args.batch_train, shuffle=True)\n",
        "        test_loader = DataLoader(test_subset, batch_size=args.batch_train, shuffle=False)\n",
        "\n",
        "        yield train_loader, test_loader\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mean = [0.1307]\n",
        "    std = [0.3081]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "    dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "    dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(dst_train, batch_size=args.batch_train, shuffle=True)\n",
        "    test_loader = DataLoader(dst_test, batch_size=args.batch_train, shuffle=False)\n",
        "\n",
        "    result_list = []\n",
        "    for e in [1, 5, 10, 20]:\n",
        "        for lr in [0.01, 0.1, 1.0]:\n",
        "            for batch_size in [128, 256, 512]:\n",
        "                args.epochs = e\n",
        "                args.lr_net = lr\n",
        "                args.batch_train = batch_size\n",
        "                acc = train(train_loader, test_loader, args)\n",
        "                result_list.append(f'epochs = {e}, lr = {lr}, batch_size = {batch_size}, acc = {acc:.3f}')\n",
        "    print(result_list)\n",
        "\n",
        "    for ratio in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "        print(f'ratio = {ratio}')\n",
        "        train_loader, test_loader = hold_out(ratio, args)\n",
        "        acc = train(train_loader, test_loader, args)\n",
        "        result_list.append(f'{ratio}: {acc:.3f}')\n",
        "    print(result_list)\n",
        "\n",
        "    for k in [5, 10, 20]:\n",
        "        buf = []\n",
        "        for train_loader, test_loader in k_fold(k, args):\n",
        "            acc = train(train_loader, test_loader, args)\n",
        "            buf.append(acc)\n",
        "        mean, std = np.mean(buf), np.std(buf)\n",
        "        result_list.append([k, mean, std])\n",
        "\n",
        "    print(result_list)\n"
      ],
      "metadata": {
        "id": "CYD3D9JVTf70",
        "outputId": "259dc278-8edd-4eed-9d07-ac4ace00c0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--device DEVICE] [--lr_net LR_NET]\n",
            "                                [--batch_train BATCH_TRAIN] [--epochs EPOCHS]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-2e196961-3e0a-428f-8412-8e1abf8bbe5b.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}